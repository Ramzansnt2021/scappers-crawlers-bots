{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from os import path\n",
    "\n",
    "#################################\n",
    "# ChromeOptions options = new ChromeOptions();\n",
    "\n",
    "urls = (\"https://www.router-switch.com/hpe-rack-servers-price.html\")\n",
    "data = pd.DataFrame(columns=[\"PID\",\"Short Description\"])\n",
    "src = \"./hpe-rack-server-master.csv\" \n",
    "option = Options()\n",
    "\n",
    "option.add_argument(\"--disable-infobars\")\n",
    "option.add_argument(\"start-maximized\")\n",
    "option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "# Pass the argument 1 to allow and 2 to block\n",
    "option.add_experimental_option(\"prefs\", { \n",
    "    \"profile.default_content_setting_values.notifications\": 1 \n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=option, executable_path=\"./chromedriver\")\n",
    "\n",
    "\n",
    "for i in range(1,16):\n",
    "    driver.get(urls+\"?p=\"+str(i))\n",
    "    driver.implicitly_wait(10)\n",
    "        \n",
    "    all_sku = driver.find_elements_by_class_name('products-list')\n",
    "        \n",
    "    # print(all_sku)\n",
    "    # List for appends\n",
    "    skus = []\n",
    "    cond = []\n",
    "        \n",
    "    for sku in all_sku:\n",
    "        soup = BeautifulSoup(sku.get_attribute('innerHTML'),'html.parser')\n",
    "\n",
    "#         result_html = sku.get_attribute(\"innerHTML\")\n",
    "#         soup = BeautifulSoup(result_html, 'html.parser')\n",
    "        try:\n",
    "            h2 = soup.findAll(\"h2\", class_=\"product-name\")\n",
    "            for h in h2:\n",
    "                pid = h.find('a').text\n",
    "                print(pid)\n",
    "                skus.append(pid)\n",
    "                # print(h2)\n",
    "                # print(pid)\n",
    "        except:\n",
    "            h2 = 'None'\n",
    "    for txt in all_sku:\n",
    "    soup = BeautifulSoup(txt.get_attribute('innerHTML'),'html.parser')\n",
    "        try:\n",
    "            div=soup.findAll(\"div\", class_=\"product-shop\")\n",
    "            for d in div:\n",
    "                desc = d.find(\"div\", class_=\"desc\").text.replace(\"Condition:Brand New Sealed\\n\",\"\").strip()\n",
    "                print(desc)\n",
    "                cond.append(desc)\n",
    "\n",
    "        except:\n",
    "            div = 'None'\n",
    "        res = dict(zip(test_keys, )) \n",
    "        df = data.append({\"PID\": skus, \"Short Description\":cond}, ignore_index=True)\n",
    "        print(\"Got these many results:\",df.shape)\n",
    "        \n",
    "df.to_csv(src, index=False)\n",
    "# if path.exists(src):\n",
    "# \t# get the path to the file in the current directory\n",
    "#     src = path.realpath(src);\n",
    "\t\t\n",
    "# \t# rename the original file\n",
    "#     os.rename(src,'./hpe-rack-server-main.csv') \n",
    "# elif:\n",
    "#     os.remove(src)\n",
    "                #         try:\n",
    "#             next_page = soup.findAll('div', class_=\"pages\")\n",
    "#             next_page.click()\n",
    "#             print(next_page)\n",
    "#         except:\n",
    "#             next_page = 'None'\n",
    "# finally:\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from os import path\n",
    "\n",
    "#################################\n",
    "# ChromeOptions options = new ChromeOptions();\n",
    "\n",
    "urls = (\"https://www.router-switch.com/hpe-rack-servers-price.html\")\n",
    "data = pd.DataFrame(columns=[\"PID\",\"Short Description\"])\n",
    "src = \"./hpe-rack-server-master.csv\" \n",
    "option = Options()\n",
    "\n",
    "option.add_argument(\"--disable-infobars\")\n",
    "option.add_argument(\"start-maximized\")\n",
    "option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "# Pass the argument 1 to allow and 2 to block\n",
    "option.add_experimental_option(\"prefs\", { \n",
    "    \"profile.default_content_setting_values.notifications\": 1 \n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=option, executable_path=\"./chromedriver\")\n",
    "\n",
    "\n",
    "for i in range(1,16):\n",
    "    driver.get(urls+\"?p=\"+str(i))\n",
    "    driver.implicitly_wait(10)\n",
    "        \n",
    "    all_sku = driver.find_elements_by_class_name('products-list')\n",
    "        \n",
    "    # print(all_sku)\n",
    "    # List for appends\n",
    "    skus = []\n",
    "    cond = []\n",
    "        \n",
    "    for sku in all_sku:\n",
    "        soup = BeautifulSoup(sku.get_attribute('innerHTML'),'html.parser')\n",
    "\n",
    "#         result_html = sku.get_attribute(\"innerHTML\")\n",
    "#         soup = BeautifulSoup(result_html, 'html.parser')\n",
    "        try:\n",
    "            h2 = soup.findAll(\"h2\", class_=\"product-name\")\n",
    "            for h in h2:\n",
    "                pid = h.find('a').text\n",
    "#                 print(pid)\n",
    "                skus.append(pid)\n",
    "                # print(h2)\n",
    "                # print(pid)\n",
    "        except:\n",
    "            h2 = 'None'\n",
    "        try:\n",
    "            div=soup.findAll(\"div\", class_=\"product-shop\")\n",
    "            for d in div:\n",
    "                desc = d.find(\"div\", class_=\"desc\").text.replace(\"Condition:Brand New Sealed\\n\",\"\").strip()\n",
    "                print(desc)\n",
    "                cond.append(desc)\n",
    "                #df = data.append({\"PID\": pid, \"Short Description\":desc }, ignore_index=True)\n",
    "                #print(\"Got these many results:\",df.shape)\n",
    "        except:\n",
    "            div = 'None'    \n",
    "    \n",
    "        \n",
    "#df.to_csv(src, index=False)\n",
    "# if path.exists(src):\n",
    "# \t# get the path to the file in the current directory\n",
    "#     src = path.realpath(src);\n",
    "\t\t\n",
    "# \t# rename the original file\n",
    "#     os.rename(src,'./hpe-rack-server-main.csv') \n",
    "# elif:\n",
    "#     os.remove(src)\n",
    "                #         try:\n",
    "#             next_page = soup.findAll('div', class_=\"pages\")\n",
    "#             next_page.click()\n",
    "#             print(next_page)\n",
    "#         except:\n",
    "#             next_page = 'None'\n",
    "# finally:\n",
    "#     driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sub category: HPE ProLiant   Gen9/10 Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from os import path\n",
    "\n",
    "#################################\n",
    "# ChromeOptions options = new ChromeOptions();\n",
    "\n",
    "urls = (\"https://www.router-switch.com/hpe-proliant-dl380-servers-price.html\")\n",
    "data = pd.DataFrame(columns=[\"PID\",\"Short Description\"])\n",
    "src = \"./hpe-rack-server-master.csv\"\n",
    "\n",
    "option = Options()\n",
    "\n",
    "option.add_argument(\"--disable-infobars\")\n",
    "option.add_argument(\"start-maximized\")\n",
    "option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "# Pass the argument 1 to allow and 2 to block\n",
    "option.add_experimental_option(\"prefs\", { \n",
    "    \"profile.default_content_setting_values.notifications\": 1 \n",
    "})\n",
    "\n",
    "driver = webdriver.Chrome(chrome_options=option, executable_path=\"./chromedriver\")\n",
    "\n",
    "for i in range(1,5):\n",
    "    driver.get(urls+\"?p=\"+str(i))\n",
    "    driver.implicitly_wait(10)\n",
    "        \n",
    "    all_sku = driver.find_elements_by_class_name('products-list')\n",
    "            \n",
    "    for sku in all_sku:\n",
    "        soup = BeautifulSoup(sku.get_attribute('innerHTML'),'html.parser')\n",
    "        try:\n",
    "            h2 = soup.findAll(\"h2\", class_=\"product-name\")\n",
    "            for h in h2:\n",
    "                pid = h.find('a').text\n",
    "#                 print(pid)\n",
    "                \n",
    "        except:\n",
    "            h2 = 'None'\n",
    "        try:\n",
    "            div=soup.findAll(\"div\", class_=\"product-shop\")\n",
    "            for d in div:\n",
    "                desc = d.find(\"div\", class_=\"desc\").text.replace(\"Condition:Brand New Sealed\\n\",\"\").strip()\n",
    "                print(desc)\n",
    "                \n",
    "        except:\n",
    "            div = 'None'    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler using request library\n",
    "## Juniper Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "# csv_file = open('test.csv', 'w')\n",
    "# Set headers\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({ \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OSX 10_14_3) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/71.0.3578.98 Safari/537.36\", \"Accept\":\"text/html,application/xhtml+xml,application/xml; q=0.9,image/webp,image/apng,*/*;q=0.8\"})\n",
    "# csv_writer = csv.writer(csv_file)\n",
    "# csv_writer.writerow(['PID'])\n",
    "skus = []\n",
    "descrip = []\n",
    "# df = pd.DataFrame(columns=['PID', 'Description'])\n",
    "\n",
    "n = int(input(\"Enter a last pagination no. : \"))\n",
    "url = input(\"Give me url : \")\n",
    "\n",
    "for i in range(1,n):\n",
    "# router-switch - hpe-proliant-dl360-servers\n",
    "    \n",
    "    r = requests.get(url+\"?p=\"+str(i) , headers)\n",
    "    time.sleep(300)\n",
    "    \n",
    "# Convert to a beautiful soup object\n",
    "    soup = bs(r.content)\n",
    "    \n",
    "# Print out our html\n",
    "#     print(soup.prettify())\n",
    "\n",
    "    # detail = soup.find_all(\"div\", attrs={\"class\": \"products-list\"})\n",
    "    detail = soup.select(\".products-list\")\n",
    "#     print(detail)\n",
    "    for sku in detail:\n",
    "        #soup = BeautifulSoup(sku.get_attribute('innerHTML'),'html.parser')\n",
    "        try:\n",
    "            h2 = sku.select(\".product-name\")\n",
    "#             ptint(h2)\n",
    "            for h in h2:\n",
    "                pid = h.find('a').text\n",
    "#                 print(pid)\n",
    "                skus.append(pid)\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            pid = 'None'\n",
    "        try:\n",
    "            txt = sku.select(\".product-shop\")\n",
    "            for d in txt:\n",
    "                desc = d.find(\"div\", attrs={'class':'desc'}).text.replace(\"Condition:Brand New Sealed\\n\",\" \").strip()\n",
    "#                 print(desc)\n",
    "                descrip.append(desc.replace(\"Condition:Brand New Sealed\\r\\n\",\" \").strip().replace(\"\\r\\n\",\"\"))\n",
    "                \n",
    "        except Exception as e:\n",
    "            desc = 'None'   \n",
    "# print(\"list\",skus)\n",
    "# print(\"list\", descrip)\n",
    "df = pd.DataFrame({'PID':skus, 'Description':descrip})\n",
    "# df2 = pd.DataFrame({'Description':descrip})\n",
    "\n",
    "# print(\"DataFrame 1\", df1)\n",
    "# print(\"DataFrame 2\", df2)\n",
    "# df = pd.concat([df1, df2],  axis=1, sort=False )\n",
    "\n",
    "print(df)\n",
    "print(\"Got these many results:\",df.shape)\n",
    "                \n",
    "df.to_csv(\"D://ormusa//HPE Accessories//master.csv\",index=False)\n",
    "            \n",
    "# csv_writer.writerow([skus])\n",
    "\n",
    "# csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPE\n",
    "## Tags & Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "url = input(\"Give me urls: \")\n",
    "r = requests.get(url)\n",
    "# Convert to a beautiful soup object\n",
    "page = r.text\n",
    "soup = bs(page,'html.parser')\n",
    "attributes = []\n",
    "# Print out our html\n",
    "# print(soup.prettify())\n",
    "attribute = soup.select(\".block-content \")\n",
    "# print(attributes)\n",
    "for atr in attribute:\n",
    "    attr = atr.select(\"dl\",attrs={'id':'narrow-by-list'})\n",
    "#     print(attr)\n",
    "    for a in attr:\n",
    "        txt = a.find_all(\"a\", href=True)\n",
    "#         print(txt)\n",
    "        for href in txt:\n",
    "            link = href['href']\n",
    "            if '?server_memory=' in link:\n",
    "                print(link)\n",
    "            elif '?server_processor=' in link:\n",
    "                print(link)\n",
    "            elif '?server_power_supply=' in link:\n",
    "                print(link)\n",
    "            elif '?server_processor_type=' in link:\n",
    "                print(link)\n",
    "            elif '?server_harddrive_capacity=' in link:\n",
    "                print(link)\n",
    "            else:\n",
    "                link ='None'\n",
    "            attributes.append(link)\n",
    "print(attributes)\n",
    "df = pd.DataFrame({'Links':attributes})\n",
    "\n",
    "print(df)\n",
    "print(\"Got these many results:\",df.shape)\n",
    "                \n",
    "df.to_csv(\"D://ormusa//HPE Servers//HPE Blade Server//HPE Blade Server attibutes links.csv\",\n",
    "          index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "url = input(\"Give me urls: \")\n",
    "r = requests.get(url)\n",
    "# Convert to a beautiful soup object\n",
    "page = r.text\n",
    "soup = bs(page,'html.parser')\n",
    "tags = []\n",
    "heads = []\n",
    "# Print out our html\n",
    "# print(soup.prettify())\n",
    "attribute = soup.select(\".block-content \")\n",
    "# print(attribute)\n",
    "for atr in attribute:\n",
    "    attr = atr.select(\"dl\",attrs={'id':'narrow-by-list'})\n",
    "#     print(attr)\n",
    "    for a in attr:\n",
    "        txt = a.find_all(\"a\", href=True)\n",
    "        dt = a.find_all(\"dt\")\n",
    "#         print(txt, dt)\n",
    "        for href in txt:\n",
    "            tag = href.text.replace(\"\\n\", \"\").strip()\n",
    "#             print(tag)\n",
    "            tags.append(tag)\n",
    "        for t in dt:\n",
    "            dt = t.text\n",
    "#             print(tag)\n",
    "            heads.append(dt)\n",
    "# print(tags)\n",
    "# print(heads)\n",
    "\n",
    "heads.extend(tags)\n",
    "# print(heads)\n",
    "aFile = open(\"D://ormusa//HPE Servers//HPE Blade Server//HPE Blade Server attibutes data.txt\",\"w+\")\n",
    "aFile.writelines(heads) \n",
    "aFile.close()\n",
    "\n",
    "# with open('D://ormusa//HPE Servers//HPE Rack Server attributes data.csv', 'a+') as csv_file:\n",
    "#     csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "#     csv_writer.writerow(attributes)\n",
    "# csv_file.close() \n",
    "        \n",
    "#     .replace(\"\\n\\n\",\"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CISCO HPE DELL \n",
    "### ID and Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# csv_file = open('test.csv', 'w')\n",
    "# Set headers\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({ 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'})\n",
    "# csv_writer = csv.writer(csv_file)\n",
    "# csv_writer.writerow(['PID'])\n",
    "skus = []\n",
    "gpl = []\n",
    "prices = []\n",
    "# df = pd.DataFrame(columns=['PID', 'Description'])\n",
    "\n",
    "n = int(input(\"Enter a last pagination no. : \"))\n",
    "url = input(\"Give me url : \")\n",
    "for i in range(1,n):\n",
    "# router-switch - hpe-proliant-dl360-servers\n",
    "    r = requests.get(url+\"?p=\"+str(i) , headers)\n",
    "\n",
    "# Convert to a beautiful soup object\n",
    "    soup = bs(r.content)\n",
    "\n",
    "# Print out our html\n",
    "    # print(soup.prettify())\n",
    "\n",
    "    # detail = soup.find_all(\"div\", attrs={\"class\": \"products-list\"})\n",
    "    detail = soup.select(\".products-list\")\n",
    "    # print(detail)\n",
    "    for sku in detail:\n",
    "        #soup = BeautifulSoup(sku.get_attribute('innerHTML'),'html.parser')\n",
    "        try:\n",
    "            h2 = sku.select(\".product-name\")\n",
    "            for h in h2:\n",
    "                pid = h.find('a').text\n",
    "#                 print(pid)\n",
    "                skus.append(pid)\n",
    "                \n",
    "                \n",
    "        except Exception as e:\n",
    "            pid = 'None'\n",
    "        try:\n",
    "            txt = sku.select(\".price-box-gpl\")\n",
    "            for d in txt:\n",
    "                desc = d.find(\"span\", attrs={'class':'price'}).text\n",
    "                gpl.append(desc)\n",
    "                \n",
    "        except Exception as e:\n",
    "            desc = 'None' \n",
    "            \n",
    "        try:\n",
    "            txt = sku.select(\".price-box\")\n",
    "            for d in txt:\n",
    "                desc = d.find(\"span\", attrs={'class':'price'}).text\n",
    "                prices.append(desc)\n",
    "                \n",
    "        except Exception as e:\n",
    "            desc = 'None' \n",
    "            \n",
    "# print(\"list\",skus)\n",
    "# print(\"list\", descrip)\n",
    "\n",
    "df = pd.DataFrame({'PID':skus, 'GPL-Price':gpl, 'Regular-Prices':prices})\n",
    "\n",
    "# df2 = pd.DataFrame({'Description':descrip})\n",
    "\n",
    "# print(\"DataFrame 1\", df1)\n",
    "# print(\"DataFrame 2\", df2)\n",
    "# df = pd.concat([df1, df2],  axis=1, sort=False )\n",
    "\n",
    "print(df)\n",
    "print(\"Got these many results:\",df.shape)\n",
    "                \n",
    "# df.to_csv(\"D://ormusa//HPE Switches//HPE ProCurve Switches//HPE ProCurve Switches master.csv\",\n",
    "#           index=False)\n",
    "            \n",
    "# csv_writer.writerow([skus])\n",
    "\n",
    "# csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORM SYSTEMS UK\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# csv_file = open('test.csv', 'w')\n",
    "# Set headers\n",
    "headers = requests.utils.default_headers()\n",
    "headers.update({ 'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0'})\n",
    "# csv_writer = csv.writer(csv_file)\n",
    "# csv_writer.writerow(['PID'])\n",
    "skus = []\n",
    "descrip = []\n",
    "# df = pd.DataFrame(columns=['PID', 'Description'])\n",
    "\n",
    "n = int(input(\"Enter a last pagination no. : \"))\n",
    "url = input(\"Give me url : \")\n",
    "for i in range(1,n):\n",
    "# router-switch - hpe-proliant-dl360-servers\n",
    "    r = requests.get(url+\"?page=\"+str(i) , headers)\n",
    "\n",
    "# Convert to a beautiful soup object\n",
    "    soup = bs(r.content)\n",
    "\n",
    "# Print out our html\n",
    "    print(soup.prettify())\n",
    "#     allCate = soup.find('div', class_ = \"categori-item-area\")\n",
    "#     print(allCate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Juniper bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0dfe3beb2841>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-0dfe3beb2841>\"\u001b[1;36m, line \u001b[1;32m34\u001b[0m\n\u001b[1;33m    urls =\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from os import path\n",
    "\n",
    "#################################\n",
    "# ChromeOptions options = new ChromeOptions();\n",
    "# Counter\n",
    "counter = int(input('Make a threads of bot more than 1:'))\n",
    "while counter >= 1: \n",
    "    option = Options()\n",
    "    option.add_argument(\"--disable-infobars\")\n",
    "    option.add_argument(\"start-maximized\")\n",
    "    option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    # Pass the argument 1 to allow and 2 to block\n",
    "    option.add_experimental_option(\"prefs\", { \n",
    "        \"profile.default_content_setting_values.notifications\": 1 \n",
    "    })\n",
    "\n",
    "    driver = webdriver.Chrome(chrome_options=option, executable_path=\"./chromedriver\")\n",
    "\n",
    "    source = \"D://ormusa//HPE Storage\"\n",
    "    skus = []\n",
    "    descrip = []\n",
    "    gpl = []\n",
    "    prices = []\n",
    "\n",
    "    n = int(input(\"Enter a last pagination no. : \"))\n",
    "    urls = input(\"Give me url : \")\n",
    "    for i in range(1,n):\n",
    "        driver.get(urls+\"?p=\"+str(i))\n",
    "        driver.implicitly_wait(30)\n",
    "        \n",
    "        all_sku = driver.find_elements_by_class_name('products-list')\n",
    "    \n",
    "    \n",
    "        for sku in all_sku:\n",
    "            soup = BeautifulSoup(sku.get_attribute('innerHTML'),'html.parser')\n",
    "#         print(soup.prettify()) https://www.router-switch.com/juniper-price.html\n",
    "            try:\n",
    "                h2 = soup.select(\".product-name\")\n",
    "#             print(h2)\n",
    "                for h in h2:\n",
    "                    pid = h.find('a').text\n",
    "                    skus.append(pid)\n",
    "            except Exception as e:\n",
    "                h2 = 'None'\n",
    "            try:\n",
    "                txt = soup.select(\".product-shop\")\n",
    "                for d in txt:\n",
    "                    desc = d.find(\"div\", attrs={'class':'desc'}).text.replace(\"Condition:Brand New Sealed\\n\",\" \").strip()\n",
    "#                 print(desc)\n",
    "                    descrip.append(desc.replace(\"Condition:Brand New Sealed\\r\\n\",\" \").strip().replace(\"\\n\",\" \"))\n",
    "                \n",
    "            except Exception as e:\n",
    "                desc = 'None'\n",
    "            \n",
    "            try:\n",
    "                txt = soup.select(\".price-box-gpl\")\n",
    "                for d in txt:\n",
    "                    desc = d.find(\"span\", attrs={'class':'price'}).text\n",
    "                    gpl.append(desc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                desc = 'None' \n",
    "            \n",
    "            try:\n",
    "                txt = soup.select(\".price-box\")\n",
    "                for d in txt:\n",
    "                    desc = d.find(\"span\", attrs={'class':'price'}).text\n",
    "                    prices.append(desc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                desc = 'None' \n",
    "# print('Products ID', skus)\n",
    "# print('Products Short Description',descrip)\n",
    "# print('New',prices)\n",
    "# print('OLD',gpl)\n",
    "    data = {'PID':skus, 'Description':descrip, 'OLD Price':gpl, 'SALE Price':prices}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.transpose()\n",
    "    print(df)\n",
    "# print(\"Got these many results:\",df.shape)\n",
    "\n",
    "# Juniper QFX5100 Series Switches |  D://ormusa//JUNIPER FILES//juniper switches//Juniper QFX5100 Series Switches.csv\n",
    "# Juniper EX2300 Series Ethernet Switches |  D://ormusa//JUNIPER FILES//juniper switches//Juniper QFX5100 Series Switches.csv\n",
    "# Juniper EX4300 Series Ethernet Switches |  D://ormusa//JUNIPER FILES//juniper switches//Juniper QFX5100 Series Switches.csv\n",
    "# Juniper EX3400 Series Ethernet Switches |  D://ormusa//JUNIPER FILES//juniper switches//Juniper QFX5100 Series Switches.csv\n",
    "# Juniper EX4600 Series Ethernet Switches |  D://ormusa//JUNIPER FILES//juniper switches//Juniper QFX5100 Series Switches.csv\n",
    "# Juniper Switch Accessories |  D://ormusa//JUNIPER FILES//juniper switches//Juniper QFX5100 Series Switches.csv\n",
    "# Juniper Switch Modules & Cards |  D://ormusa//JUNIPER FILES//juniper switches//Juniper QFX5100 Series Switches.csv\n",
    "# Juniper QFX5100 Series Switches |  D://ormusa//JUNIPER FILES//juniper switches//Juniper QFX5100 Series Switches.csv\n",
    "\n",
    "\n",
    "    file_name = input(\"Name of the file using CSV extention e.g; master files.csv: \")\n",
    "    df.to_csv(source+\"\"+file_name,index=False)\n",
    "    \n",
    "    counter = counter - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from os import path\n",
    "\n",
    "#################################\n",
    "# ChromeOptions options = new ChromeOptions();\n",
    "counter = int(input('Make a threads of bot more than 1:'))\n",
    "while counter >= 1: \n",
    "    if counter == 13:\n",
    "        print(\"Stopped.. run again...\")\n",
    "        exit()\n",
    "    option = Options()\n",
    "    option.add_argument(\"--disable-infobars\")\n",
    "    option.add_argument(\"start-maximized\")\n",
    "    option.add_argument(\"--disable-extensions\")\n",
    "\n",
    "    # Pass the argument 1 to allow and 2 to block\n",
    "    option.add_experimental_option(\"prefs\", { \n",
    "        \"profile.default_content_setting_values.notifications\": 1 \n",
    "    })\n",
    "\n",
    "    driver = webdriver.Chrome(chrome_options=option, executable_path=\"./chromedriver\")\n",
    "\n",
    "    skus = []\n",
    "    source = \"D://ormusa//cisco files//prices//\"\n",
    "    gpl = []\n",
    "    prices = []\n",
    "\n",
    "    n = int(input(\"Enter a last pagination no. : \"))\n",
    "    urls = input(\"Give me url : \")\n",
    "    for i in range(1,n):\n",
    "        driver.get(urls+\"?p=\"+str(i))\n",
    "        driver.implicitly_wait(30)\n",
    "        \n",
    "        all_sku = driver.find_elements_by_class_name('products-list')\n",
    "    \n",
    "    \n",
    "        for sku in all_sku:\n",
    "            soup = BeautifulSoup(sku.get_attribute('innerHTML'),'html.parser')\n",
    "    #         print(soup.prettify()) https://www.router-switch.com/juniper-price.html\n",
    "            try:\n",
    "                h2 = soup.select(\".product-name\")\n",
    "    #             print(h2)\n",
    "                for h in h2:\n",
    "                    pid = h.find('a').text\n",
    "                    skus.append(pid)\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                h2 = 'None'\n",
    "            try:\n",
    "                txt = soup.select(\".price-box-gpl\")\n",
    "                for d in txt:\n",
    "                    desc = d.find(\"span\", attrs={'class':'price'}).text\n",
    "                    gpl.append(desc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                desc = 'None' \n",
    "            \n",
    "            try:\n",
    "                txt = soup.select(\".price-box\")\n",
    "                for d in txt:\n",
    "                    desc = d.find(\"span\", attrs={'class':'price'}).text\n",
    "                    prices.append(desc)\n",
    "                \n",
    "            except Exception as e:\n",
    "                desc = 'None' \n",
    "    # print('Products ID', skus)\n",
    "\n",
    "    # print('New',prices)\n",
    "    # print('OLD',gpl)\n",
    "    data = {'PID':skus, 'OLD Price':gpl, 'SALE Price':prices}\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    df = df.transpose()\n",
    "    print(df)\n",
    "    print(\"Got these many results:\",df.shape)\n",
    "\n",
    "    file_name = input(\"Name of the file using CSV extention e.g; master files.csv: \")\n",
    "    df.to_csv(source+\"\"+file_name,index=False)\n",
    "    \n",
    "    counter = counter - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import csv\n",
    "\n",
    "class ProxyScraper:\n",
    "    results = []\n",
    "\n",
    "    def fetch(self, url):\n",
    "        return requests.get(url)\n",
    "\n",
    "    def parse(self, html):\n",
    "        content = BeautifulSoup(html, 'lxml')\n",
    "        table = content.find('table')\n",
    "        rows = table.findAll('tr')\n",
    "        headers = [header.text for header in rows[0]]\n",
    "        results = [headers]\n",
    "\n",
    "       \n",
    "        for row in rows:\n",
    "            if len(row.findAll('td')):\n",
    "                self.results.append([data.text for data in row.findAll('td')])\n",
    "\n",
    "    def to_csv(self):\n",
    "        with open('proxies.csv', 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerows(self.results)\n",
    "    \n",
    "    def run(self):\n",
    "        response = self.fetch('https://www.free-proxy-list.net/')\n",
    "        self.parse(response.text)\n",
    "        self.to_csv()\n",
    "        \n",
    "scraper = ProxyScraper()\n",
    "print(scraper.results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
